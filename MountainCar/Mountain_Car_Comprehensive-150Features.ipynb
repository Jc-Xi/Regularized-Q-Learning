{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a156d246-688c-465e-a4d7-710da2ebc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from scipy.special import logsumexp,softmax\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ab3bda-e6ae-4ca5-a47c-7db76ae16f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "num_episodes = 1000\n",
    "discount_factor = 1.0\n",
    "n_features = 50\n",
    "\n",
    "nA = env.action_space.n\n",
    "\n",
    "\n",
    "# Get satistics over observation space samples for normalization\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Normalize and turn into feature\n",
    "def featurize_state(state):\n",
    "    # Transform data\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized\n",
    "\n",
    "def Q(state,action,weight):\n",
    "    value = state.dot(weight[action])\n",
    "    return value\n",
    "def max_Q(state,weight):\n",
    "\tvalue = np.max([Q(state,a,weight) for a in range(nA)])\n",
    "\treturn value\n",
    "def softmax_Q(state,weight,tau = 0.05):\n",
    "    return logsumexp([Q(state,a,weight)/tau for a in range(nA)])\n",
    "\n",
    "def greedy_policy(state, weight,epsilon=0.1):\n",
    "    A = np.ones(nA,dtype=float) * epsilon/nA\n",
    "    best_action =  np.argmax([Q(state,a,weight) for a in range(nA)])\n",
    "    A[best_action] += (1.0-epsilon)\n",
    "    sample = np.random.choice(nA,p=A)\n",
    "    return sample\n",
    "\n",
    "def greedy_policy_eva(state, weight):\n",
    "    best_action =  np.argmax([Q(state,a,weight) for a in range(nA)])\n",
    "    return best_action\n",
    "\n",
    "def softmax_policy(state,weight,epsilon=0.1,tau = 0.05):\n",
    "    A = np.ones(nA,dtype=float) * epsilon/nA\n",
    "    A += (1.0-epsilon)*softmax([Q(state,a,weight)/tau for a in range(nA)]).flatten()\n",
    "    sample = np.random.choice(nA,p=A)\n",
    "    return sample\n",
    "\n",
    "def softmax_policy_eva(state,weight,tau = 0.05):\n",
    "    A = softmax([Q(state,a,weight)/tau for a in range(nA)]).flatten()\n",
    "    sample = np.random.choice(nA,p=A)\n",
    "    return sample\n",
    "\n",
    "def policy_dist(state,action,weight,tau = 0.05):\n",
    "    A = softmax([Q(state,a,weight)/tau for a in range(nA)]).flatten()\n",
    "    return A[action]\n",
    "def policy_grad(state,action,weight,tau =0.05):\n",
    "    dist = policy_dist(state,action,weight,tau)\n",
    "    return dist*state\n",
    "def V(state,weight,tau = 0.05):\n",
    "    return np.sum([Q(state,a,weight).dot(policy_dist(weight,a,weight,tau)) for a in range(nA)])\n",
    "def max_double_Q(state,weight1,weight2):\n",
    "    best_action =  np.argmax([Q(state,a,weight1) for a in range(nA)])\n",
    "    value = Q(state,best_action,weight2)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed2981-39de-494e-8776-d7641f12491d",
   "metadata": {},
   "source": [
    "### 150 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a76cd3-c5ad-42ba-a7d1-6c64597bbac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FeatureUnion(transformer_list=[(&#x27;rbf&#x27;,\n",
       "                                RBFSampler(n_components=50, random_state=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;rbf&#x27;,\n",
       "                                RBFSampler(n_components=50, random_state=1))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rbf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RBFSampler</label><div class=\"sk-toggleable__content\"><pre>RBFSampler(n_components=50, random_state=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "FeatureUnion(transformer_list=[('rbf',\n",
       "                                RBFSampler(n_components=50, random_state=1))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create radial basis function sampler to convert states to features for nonlinear function approx\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf\", RBFSampler(gamma=1.0,random_state = 1, n_components=n_features))\n",
    "        ])\n",
    "\n",
    "# Fit featurizer to our scaled inputs\n",
    "featurizer.fit(scaler.transform(observation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f21322-754c-42ed-9f5c-1fd2bdd17151",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = {}\n",
    "seed_range = np.arange(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8022d0b0-db4e-4498-8aed-b1c609e3a8a4",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839f69c1-869d-4d55-a31e-55c43f79c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model['Q-Learning'] = []\n",
    "for seed in seed_range:\n",
    "    np.random.seed(seed)\n",
    "    alpha = 0.01\n",
    "    w = np.zeros((nA,n_features))\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = greedy_policy(state,w)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Statistic for graphing\n",
    "            episode_rewards[e] += reward\n",
    "            # Figure out target and td error\n",
    "            target = reward + discount_factor * max_Q(next_state,w)\t\t\n",
    "            td_error = Q(state,action,w) - target\n",
    "            # Find gradient with code to check it commented below (check passes)\n",
    "            dw = (td_error).dot(state)\n",
    "            # Update weight\n",
    "            w[action] -= alpha * dw\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "    saved_model['Q-Learning'].append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe4ef1-dc10-418a-8766-5d60e737b78e",
   "metadata": {},
   "source": [
    "#### Our Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55fd6217-989c-4b47-9e51-7e438c3e96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "tau = 0.05\n",
    "threshold = 500\n",
    "def truncate(x,threshold):\n",
    "    return threshold* np.tanh(x/threshold)\n",
    "saved_model['Our Algorithm'] = []\n",
    "for seed in seed_range:\n",
    "    np.random.seed(seed)\n",
    "    theta = np.zeros((nA,n_features))\n",
    "    omega = np.zeros((nA,n_features))\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = softmax_policy_eva(state,theta)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Update omega\n",
    "            target = reward + discount_factor * truncate(tau*softmax_Q(next_state,theta),threshold)\n",
    "            td_error = Q(state,action,omega)-target\n",
    "            domega = td_error.dot(state)\n",
    "            omega[action] -= beta*domega\n",
    "            # Update theta\n",
    "            Q_diff = Q(state,action,omega-theta)[0]\n",
    "            grad_tanh = discount_factor * (1- np.square(truncate(tau*softmax_Q(next_state,theta),threshold))/(threshold**2))\n",
    "    \n",
    "            dtheta = (grad_tanh*policy_dist(next_state,action,theta)*next_state[0] - state[0])*Q_diff/np.linalg.norm(omega[action]-theta[action])\n",
    "            theta[action] -= alpha*dtheta\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "    saved_model['Our Algorithm'].append(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64286ce3-5aab-4f19-a4b7-58ac9aaae9c7",
   "metadata": {},
   "source": [
    "#### Our Algorithm $\\tau = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58d0c8db-f578-4a43-8ff6-875180cb956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "tau = 0.01\n",
    "threshold = 500\n",
    "def truncate(x,threshold):\n",
    "    return threshold* np.tanh(x/threshold)\n",
    "saved_model['Our Algorithm 0.01'] = []\n",
    "for seed in seed_range:\n",
    "    np.random.seed(seed)\n",
    "    theta = np.zeros((nA,n_features))\n",
    "    omega = np.zeros((nA,n_features))\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = softmax_policy_eva(state,theta,tau = tau)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Update omega\n",
    "            target = reward + discount_factor * truncate(tau*softmax_Q(next_state,theta,tau = tau),threshold)\n",
    "            td_error = Q(state,action,omega)-target\n",
    "            domega = td_error.dot(state)\n",
    "            omega[action] -= beta*domega\n",
    "            # Update theta\n",
    "            Q_diff = Q(state,action,omega-theta)[0]\n",
    "            grad_tanh = discount_factor * (1- np.square(truncate(tau*softmax_Q(next_state,theta,tau = tau),threshold))/(threshold**2))\n",
    "    \n",
    "            dtheta = (grad_tanh*policy_dist(next_state,action,theta,tau = tau)*next_state[0] - state[0])*Q_diff/np.linalg.norm(omega[action]-theta[action])\n",
    "            theta[action] -= alpha*dtheta\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "    saved_model['Our Algorithm 0.01'].append(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb37201-e3df-4d4b-b447-2a54d834ef4a",
   "metadata": {},
   "source": [
    "#### Our Algorithm $\\tau = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaf524b1-22e1-478d-bedd-fe379a5f8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "tau = 0.1\n",
    "threshold = 500\n",
    "def truncate(x,threshold):\n",
    "    return threshold* np.tanh(x/threshold)\n",
    "saved_model['Our Algorithm 0.1'] = []\n",
    "for seed in seed_range:\n",
    "    np.random.seed(seed)\n",
    "    theta = np.zeros((nA,n_features))\n",
    "    omega = np.zeros((nA,n_features))\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = softmax_policy_eva(state,theta,tau = tau)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Update omega\n",
    "            target = reward + discount_factor * truncate(tau*softmax_Q(next_state,theta,tau = tau),threshold)\n",
    "            td_error = Q(state,action,omega)-target\n",
    "            domega = td_error.dot(state)\n",
    "            omega[action] -= beta*domega\n",
    "            # Update theta\n",
    "            Q_diff = Q(state,action,omega-theta)[0]\n",
    "            grad_tanh = discount_factor * (1- np.square(truncate(tau*softmax_Q(next_state,theta,tau = tau),threshold))/(threshold**2))\n",
    "    \n",
    "            dtheta = (grad_tanh*policy_dist(next_state,action,theta,tau = tau)*next_state[0] - state[0])*Q_diff/np.linalg.norm(omega[action]-theta[action])\n",
    "            theta[action] -= alpha*dtheta\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "    saved_model['Our Algorithm 0.1'].append(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf2de7-770c-47f0-b519-bd9a78aaec40",
   "metadata": {},
   "source": [
    "#### Coupled Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0ddc31d-b39e-4581-963d-918df734b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "saved_model['CQL'] = []\n",
    "for seed in seed_range:\n",
    "    np.random.seed(seed)\n",
    "    u = np.zeros((nA,n_features))\n",
    "    v = np.zeros((nA,n_features))\n",
    "    for e in range(num_episodes):\n",
    "    \n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "    \n",
    "        for j in range(200):\n",
    "    \n",
    "            #env.render()\n",
    "            # Sample from our policy\n",
    "            action = greedy_policy(state,v)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "    \n",
    "            #u update\n",
    "            du = Q(state,action,v).dot(state)-u[action]\n",
    "            u[action] += alpha*du\n",
    "            #v update\n",
    "            target = reward + discount_factor * max_Q(next_state,u)\t\t\n",
    "            td = target -Q(state,action,v) \n",
    "            dv = td.dot(state)\n",
    "            v[action] += beta*dv\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "    saved_model['CQL'].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cace9ad-564a-444c-9337-c5aa5355ec93",
   "metadata": {},
   "source": [
    "#### Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3273e397-f194-47ee-94e5-5de19b2ab85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "saved_model['DQL'] = []\n",
    "for seed in seed_range:\n",
    "    np.random.seed(seed)\n",
    "    u = np.zeros((nA,n_features))\n",
    "    v = np.zeros((nA,n_features))\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "    \n",
    "            #env.render()\n",
    "            # Sample from our policy\n",
    "            action = greedy_policy(state,u)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "    \n",
    "            #Update\n",
    "            rn = np.random.choice(2)\n",
    "            if rn == 0: #update u\n",
    "                target = reward + discount_factor * max_double_Q(next_state,u,v)\n",
    "                td = target-Q(state,action,u)\n",
    "                du = td.dot(state)\n",
    "                u[action] += alpha* du\n",
    "            if rn == 1: #update v\n",
    "                target = reward + discount_factor * max_double_Q(next_state,v,u)\n",
    "                td = target-Q(state,action,v)\n",
    "                dv = td.dot(state)\n",
    "                v[action] += alpha* dv\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "    saved_model['DQL'].append(u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7346a05-8209-484c-9f81-ae039a283a80",
   "metadata": {},
   "source": [
    "#### TARGET NETWORK AND TRUNCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6046eb06-82f2-4ab8-a988-ec5347d05ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jx3297/anaconda3/envs/torch_gpu/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "saved_model['Target_Truncation'] = []\n",
    "for seed in seed_range:\n",
    "    np.random.seed(seed)\n",
    "    alpha = 0.01\n",
    "    w = np.zeros((nA,n_features))\n",
    "    u = w.copy()\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = greedy_policy(state,w)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Statistic for graphing\n",
    "            episode_rewards[e] += reward\n",
    "            # Figure out target and td error\n",
    "            target = reward + discount_factor * np.minimum(np.maximum(max_Q(next_state,u),-200),200)\t\t\n",
    "            td_error = Q(state,action,w) - target\n",
    "            # Find gradient with code to check it commented below (check passes)\n",
    "            dw = (td_error).dot(state)\n",
    "            # Update weight\n",
    "            w[action] -= alpha * dw\n",
    "            if done:\n",
    "                break\n",
    "            if (j+1)% 20 ==0:\n",
    "                u = w.copy()\n",
    "            # update our state\n",
    "            state = next_state\n",
    "    saved_model['Target_Truncation'].append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91bdc0-1ae7-4031-8789-9752bd758b5e",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ddec82-7730-4345-be09-e79fac31a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance = {}\n",
    "n_traj_per_seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0db0109-533c-46be-81c4-cb5768bb0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance['Q-Learning'] = []\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "for w in saved_model['Q-Learning']:\n",
    "    for k in range(n_traj_per_seed):\n",
    "        traj_reward = 0\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = greedy_policy_eva(state,w)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Statistic for graphing\n",
    "            traj_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "        Performance['Q-Learning'].append(traj_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8803ce6-f1cd-4d8e-9318-c688be0d390b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-141.035"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Performance['Q-Learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c6d1c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.765172622051313"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(Performance['Q-Learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73db9f92-3e74-41c1-b577-80b6081627aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance['Our Algorithm'] = []\n",
    "tau = 0.05\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "for theta in saved_model['Our Algorithm']:\n",
    "    for k in range(n_traj_per_seed):\n",
    "        traj_reward = 0\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = softmax_policy_eva(state,theta)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Statistic for graphing\n",
    "            traj_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "        Performance['Our Algorithm'].append(traj_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22c0c669-b8e0-470d-9322-8452e4445713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-120.165"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Performance['Our Algorithm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e97e4802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.641651460093982"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(Performance['Our Algorithm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e75812f-246b-4380-94ed-f1f6680d3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.01\n",
    "Performance['Our Algorithm 0.01'] = []\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "for theta in saved_model['Our Algorithm 0.01']:\n",
    "    for k in range(n_traj_per_seed):\n",
    "        traj_reward = 0\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = softmax_policy_eva(state,theta,tau =tau)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Statistic for graphing\n",
    "            traj_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "        Performance['Our Algorithm 0.01'].append(traj_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5968e0d8-aacc-4c72-b3d2-5df9ec125f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-139.58"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Performance['Our Algorithm 0.01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3c34db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.58600495482307"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(Performance['Our Algorithm 0.01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2868f45f-9d34-423a-950c-606ecc20b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.1\n",
    "Performance['Our Algorithm 0.1'] = []\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "for theta in saved_model['Our Algorithm 0.1']:\n",
    "    for k in range(n_traj_per_seed):\n",
    "        traj_reward = 0\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = softmax_policy_eva(state,theta,tau =tau)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Statistic for graphing\n",
    "            traj_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "        Performance['Our Algorithm 0.1'].append(traj_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb2ab734-a9af-42f1-9449-558f25c219b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-131.56"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Performance['Our Algorithm 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19a551a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.28137903285481"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(Performance['Our Algorithm 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f89f2107-bf06-4dd8-b88b-b2bbe74e399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance['CQL'] = []\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "for v in saved_model['CQL']:\n",
    "    for k in range(n_traj_per_seed):\n",
    "        traj_reward = 0\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = greedy_policy_eva(state,v)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Statistic for graphing\n",
    "            traj_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "        Performance['CQL'].append(traj_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6018bb30-9345-46ae-9de0-cc54ae6161e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-200.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Performance['CQL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "088eb536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(Performance['CQL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "478e4348-5be9-4fd1-8880-c9e8e5464ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance['DQL'] = []\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "for u in saved_model['DQL']:\n",
    "    for k in range(n_traj_per_seed):\n",
    "        traj_reward = 0\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = greedy_policy_eva(state,u)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Statistic for graphing\n",
    "            traj_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "        Performance['DQL'].append(traj_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa66dc60-4927-4c13-8653-980e1337e210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-155.595"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Performance['DQL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ba0b578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.43144294693526"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(Performance['DQL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75f87772-95f9-4b9f-aef3-efd0d6ed1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance['Target_Truncation'] = []\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "for w in saved_model['Target_Truncation']:\n",
    "    for k in range(n_traj_per_seed):\n",
    "        traj_reward = 0\n",
    "        state = env.reset()[0]\n",
    "        state = featurize_state(state)\n",
    "        for j in range(200):\n",
    "            # Sample from our policy\n",
    "            action = greedy_policy_eva(state,w)\n",
    "            # Step environment and get next state and make it a feature\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            next_state = featurize_state(next_state)\n",
    "            # Statistic for graphing\n",
    "            traj_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            # update our state\n",
    "            state = next_state\n",
    "        Performance['Target_Truncation'].append(traj_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6def8820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-140.46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Performance['Target_Truncation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a2f5a7-d503-4c21-8cdb-c48de9e80480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.304912088809573"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(Performance['Target_Truncation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fd9ed-7e28-42ce-9b3f-d56038e57075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
